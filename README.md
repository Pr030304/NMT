Machine Translation System for India
This project implements and experiments with sequence-to-sequence architectures for cross‑lingual translation. It compares recurrent models (RNN/LSTM) with Transformer encoder–decoder architectures leveraging self‑attention for global context modeling.
