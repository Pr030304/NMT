{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13618020,"sourceType":"datasetVersion","datasetId":8654378},{"sourceId":13636866,"sourceType":"datasetVersion","datasetId":8667995},{"sourceId":631957,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":476331,"modelId":492252}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport random\nimport math\nimport collections\nimport heapq\nimport itertools\nimport unicodedata\nimport matplotlib.pyplot as plt\nimport time\nimport pandas as pd\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed(42)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"id":"AgVWXbu_a920","colab":{"base_uri":"https://localhost:8080/"},"outputId":"aa3baa40-5d21-442f-a253-272c5242febe","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:54:41.168279Z","iopub.execute_input":"2025-11-06T16:54:41.168548Z","iopub.status.idle":"2025-11-06T16:54:45.219638Z","shell.execute_reply.started":"2025-11-06T16:54:41.168519Z","shell.execute_reply":"2025-11-06T16:54:45.218844Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nGPU: Tesla P100-PCIE-16GB\nMemory: 17.06 GB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#inspired from assignemnt 2\nimport unicodedata\nimport collections\nimport heapq\nimport itertools\nimport numpy as np\n\ndef normalize_text(text):\n   \n    text = unicodedata.normalize(\"NFKC\", text)\n    return text\n\ndef pair_lex(pair):\n\n    return pair[0] + \"\\0\" + pair[1]\n\ndef get_word_freqs(text):\n   \n    words = text.split()\n    return collections.Counter(\"‚ñÅ\" + w for w in words if w)\n\ndef get_initial_splits(word_freqs):\n    \"\"\"\n    Convert words to character sequences\n    NO end-of-word markers - pure SentencePiece style\n    \"\"\"\n    splits = {}\n    for word in word_freqs:\n        if not word:\n            continue\n        # Split into characters (‚ñÅ is part of the word)\n        splits[word] = list(word)\n    return splits\n\ndef train_custom_sp_tokenizer(text, vocab_size, add_special_tokens=True):\n   \n    print(f\"üîß Training tokenizer with vocab_size={vocab_size}...\")\n    \n    word_freqs = get_word_freqs(text)\n    splits = get_initial_splits(word_freqs)\n    \n    # Cache word list for faster iteration\n    word_list = list(word_freqs.keys())\n    \n    # Base vocabulary from all unique characters\n    base_symbols = set()\n    for symbols in splits.values():\n        base_symbols.update(symbols)\n    \n    # Only 4 special tokens: <pad>, <unk>, <s>, </s>\n    num_special = 4 if add_special_tokens else 0\n    base_vocab_size = len(base_symbols)\n    merges_needed = vocab_size - num_special - base_vocab_size\n    \n    print(f\"   Base vocab: {base_vocab_size} symbols\")\n    print(f\"   Merges needed: {merges_needed}\")\n    \n    if merges_needed <= 0:\n        vocab = ([\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"] if add_special_tokens else [])\n        vocab.extend(sorted(base_symbols))\n        return vocab[:vocab_size], {}\n    \n    # Build initial pair frequencies\n    pair_freqs = collections.Counter()\n    pair_to_positions = collections.defaultdict(list)  # pair -> [(word_idx, pos)]\n    \n    for word_idx, (word, symbols) in enumerate(splits.items()):\n        freq = word_freqs[word]\n        for i in range(len(symbols) - 1):\n            pair = (symbols[i], symbols[i+1])\n            pair_freqs[pair] += freq\n            pair_to_positions[pair].append((word_idx, i))\n    \n    # Priority queue - start with ALL pairs (freq >= 1)\n    counter = itertools.count()\n    heap = [(-freq, pair_lex(pair), next(counter), pair) \n            for pair, freq in pair_freqs.items() if freq >= 1]\n    heapq.heapify(heap)\n    \n    print(f\"   Initial heap size: {len(heap)} pairs\")\n    \n    merges = {}\n    merges_done = 0\n    \n    PRINT_EVERY = 1000\n    \n    while merges_done < merges_needed and heap:\n        # Find valid best pair (lazy deletion)\n        best_pair = None\n        best_freq = 0\n        \n        while heap:\n            negf, lex_key, _, pair = heapq.heappop(heap)\n            f = -negf\n            if pair in pair_freqs and pair_freqs[pair] == f:\n                best_pair = pair\n                best_freq = f\n                break\n        \n        # Stop only if we truly can't find any more pairs\n        if not best_pair or best_freq < 1:\n            print(f\"   Stopping: no more pairs with freq >= 1\")\n            break\n        \n        a, b = best_pair\n        merged_token = a + b\n        merges[best_pair] = merged_token\n        merges_done += 1\n        \n        if merges_done % PRINT_EVERY == 0:\n            print(f\"   Progress: {merges_done}/{merges_needed} | Best freq: {best_freq}\")\n        \n        # Batch frequency updates\n        positions = pair_to_positions.pop(best_pair, [])\n        freq_deltas = {}\n        new_positions = collections.defaultdict(list)\n        \n        for word_idx, i in positions:\n            word = word_list[word_idx]\n            symbols = splits[word]\n            \n            if i >= len(symbols) - 1 or symbols[i] != a or symbols[i+1] != b:\n                continue\n            \n            freq = word_freqs[word]\n            \n            # Track left neighbor changes\n            if i > 0:\n                old_left = (symbols[i-1], symbols[i])\n                freq_deltas[old_left] = freq_deltas.get(old_left, 0) - freq\n            \n            # Track right neighbor changes\n            if i + 2 < len(symbols):\n                old_right = (symbols[i+1], symbols[i+2])\n                freq_deltas[old_right] = freq_deltas.get(old_right, 0) - freq\n            \n            # Perform merge\n            symbols[i] = merged_token\n            del symbols[i+1]\n            \n            # Track new neighbors\n            if i > 0:\n                new_left = (symbols[i-1], symbols[i])\n                freq_deltas[new_left] = freq_deltas.get(new_left, 0) + freq\n                new_positions[new_left].append((word_idx, i-1))\n            \n            if i + 1 < len(symbols):\n                new_right = (symbols[i], symbols[i+1])\n                freq_deltas[new_right] = freq_deltas.get(new_right, 0) + freq\n                new_positions[new_right].append((word_idx, i))\n        \n        # Apply frequency changes\n        for pair, delta in freq_deltas.items():\n            old_freq = pair_freqs.get(pair, 0)\n            new_freq = old_freq + delta\n            \n            if new_freq <= 0:\n                pair_freqs.pop(pair, None)\n                pair_to_positions.pop(pair, None)\n            else:\n                pair_freqs[pair] = new_freq\n                # Add pairs with frequency >= 1\n                if new_freq >= 1:\n                    heapq.heappush(heap, (-new_freq, pair_lex(pair), next(counter), pair))\n        \n        # Add new positions\n        for pair, positions_list in new_positions.items():\n            pair_to_positions[pair].extend(positions_list)\n    \n    # Build final vocabulary\n    vocab = []\n    if add_special_tokens:\n        vocab = [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]\n    \n    # Add base character symbols\n    base_tokens = sorted(base_symbols)\n    vocab.extend(base_tokens)\n    \n    # Add all merged tokens\n    merge_tokens = list(merges.values())\n    vocab.extend(merge_tokens)\n    \n    print(f\"\\nüìä Vocabulary breakdown:\")\n    print(f\"   Special tokens: {4 if add_special_tokens else 0}\")\n    print(f\"   Base symbols: {len(base_tokens)}\")\n    print(f\"   Merged tokens: {len(merge_tokens)}\")\n    print(f\"   Total before truncation: {len(vocab)}\")\n    \n    # Ensure vocab is exactly vocab_size\n    if len(vocab) > vocab_size:\n        print(f\"   ‚ö†Ô∏è  Truncating from {len(vocab)} to {vocab_size}\")\n        vocab = vocab[:vocab_size]\n    elif len(vocab) < vocab_size:\n        print(f\"   ‚ö†Ô∏è  Warning: Only {len(vocab)} tokens generated (target: {vocab_size})\")\n    \n    print(f\"‚úÖ Final vocabulary: {len(vocab)} tokens\")\n    \n    return vocab, merges\n\ndef tokenize_custom(text, merges):\n   \n    text = normalize_text(text)\n    \n    if not text:\n        return []\n    \n    # Split into words and add space markers\n    words = text.split()\n    all_tokens = []\n    \n    for word in words:\n        word = \"‚ñÅ\" + word\n        symbols = list(word)\n        \n        # Apply BPE merges\n        i = 0\n        while i < len(symbols) - 1:\n            pair = (symbols[i], symbols[i+1])\n            if pair in merges:\n                symbols[i] = merges[pair]\n                del symbols[i+1]\n                # Backtrack to check if new merge is possible\n                if i > 0:\n                    i -= 1\n            else:\n                i += 1\n        \n        all_tokens.extend(symbols)\n    \n    return all_tokens\n\ndef detokenize_custom(tokens):\n    \n    if not tokens:\n        return \"\"\n    text = \"\".join(tokens).replace(\"‚ñÅ\", \" \")\n    return text.strip()\n\nclass CustomTokenizer:\n    \n    \n    def __init__(self, vocab, merges, lang_tags=None):\n        self.vocab = list(vocab)  # Make a copy\n        self.merges = merges\n        self.token2id = {token: idx for idx, token in enumerate(self.vocab)}\n        self.id2token = {idx: token for idx, token in enumerate(self.vocab)}\n        self.vocab_size = len(self.vocab)\n        \n        # Special token IDs\n        self.pad_id = self.token2id.get(\"<pad>\", 0)\n        self.unk_id = self.token2id.get(\"<unk>\", 1)\n        self.bos_id = self.token2id.get(\"<s>\", 2)\n        self.eos_id = self.token2id.get(\"</s>\", 3)\n        \n        # Language tag IDs (if provided, add them to vocab)\n        self.lang_tag_ids = {}\n        if lang_tags:\n            for tag in lang_tags:\n                if tag in self.token2id:\n                    self.lang_tag_ids[tag] = self.token2id[tag]\n                else:\n                    # Add language tag to vocab\n                    self.token2id[tag] = self.vocab_size\n                    self.id2token[self.vocab_size] = tag\n                    self.vocab.append(tag)\n                    self.lang_tag_ids[tag] = self.vocab_size\n                    self.vocab_size += 1\n        \n        # Pre-compute special IDs set for faster lookup\n        self._special_ids = frozenset([\n            self.pad_id, self.unk_id, self.bos_id, self.eos_id\n        ] + list(self.lang_tag_ids.values()))\n    \n    def encode(self, text, add_bos=False, add_eos=False):\n       \n        tokens = tokenize_custom(text, self.merges)\n        \n        ids = []\n        if add_bos:\n            ids.append(self.bos_id)\n        \n        # Batch lookup with get (faster than exception handling)\n        unk_id = self.unk_id\n        token2id = self.token2id\n        for token in tokens:\n            ids.append(token2id.get(token, unk_id))\n        \n        if add_eos:\n            ids.append(self.eos_id)\n        \n        return ids\n    \n    def decode(self, ids, skip_special_tokens=True):\n        \"\"\"Decode IDs to text\"\"\"\n        if skip_special_tokens:\n            tokens = [self.id2token.get(idx, \"<unk>\") \n                     for idx in ids if idx not in self._special_ids]\n        else:\n            tokens = [self.id2token.get(idx, \"<unk>\") for idx in ids]\n        \n        return detokenize_custom(tokens)\n    \n    def get_lang_tag_id(self, tag):\n        \"\"\"Get ID for language tag (for prefix ID approach)\"\"\"\n        return self.lang_tag_ids.get(tag, self.unk_id)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:54:45.221219Z","iopub.execute_input":"2025-11-06T16:54:45.221650Z","iopub.status.idle":"2025-11-06T16:54:45.258726Z","shell.execute_reply.started":"2025-11-06T16:54:45.221620Z","shell.execute_reply":"2025-11-06T16:54:45.258078Z"}},"outputs":[{"name":"stdout","text":"‚úÖ CLEAN tokenizer loaded (NO </w>, PREFIX ID approach)!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print(\"\\nüìÇ Loading training data...\")\nwith open('/kaggle/input/codabenchnmt/train_data1.json', 'r', encoding='utf-8') as f:\n    train_data = json.load(f)\n\n# Clean function to remove English characters from Indic text\ndef clean_indic_text(text):\n    \n    import re\n    # Remove English letters but keep numbers and punctuation\n    text = re.sub(r'[a-zA-Z]', '', text)\n    # Remove extra spaces\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()\n\n# Prepare data WITHOUT language tags in text\nsrc_texts = []\ntgt_texts = []\nlang_tags = []\n\nLANG_TAG_HI = \"<2hi>\"\nLANG_TAG_BN = \"<2bn>\"\n\n# English-Hindi pairs\nfor key, value in train_data.get(\"English-Hindi\", {}).get(\"Train\", {}).items():\n    src = value.get(\"source\", \"\").strip()\n    tgt = value.get(\"target\", \"\").strip()\n    \n    if src and tgt:\n        tgt = clean_indic_text(tgt)\n        if tgt:  # Only add if target still has content after cleaning\n            src_texts.append(src)\n            tgt_texts.append(tgt)  # NO language tag in text\n            lang_tags.append(LANG_TAG_HI)\n\n# English-Bengali pairs\nfor key, value in train_data.get(\"English-Bengali\", {}).get(\"Train\", {}).items():\n    src = value.get(\"source\", \"\").strip()\n    tgt = value.get(\"target\", \"\").strip()\n    \n    if src and tgt:\n        tgt = clean_indic_text(tgt)\n        if tgt:\n            src_texts.append(src)\n            tgt_texts.append(tgt)  # NO language tag in text\n            lang_tags.append(LANG_TAG_BN)\n\nprint(f\"‚úÖ Loaded {len(src_texts)} sentence pairs\")\nprint(f\"   Hindi: {lang_tags.count(LANG_TAG_HI)}\")\nprint(f\"   Bengali: {lang_tags.count(LANG_TAG_BN)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZq15hqm1O9M","outputId":"2f303419-62be-41b8-dd5a-f9d7e92958b2","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:54:45.259575Z","iopub.execute_input":"2025-11-06T16:54:45.259838Z","iopub.status.idle":"2025-11-06T16:54:48.220042Z","shell.execute_reply.started":"2025-11-06T16:54:45.259814Z","shell.execute_reply":"2025-11-06T16:54:48.219357Z"}},"outputs":[{"name":"stdout","text":"\nüìÇ Loading training data...\n‚úÖ Loaded 149629 sentence pairs\n   Hindi: 80784\n   Bengali: 68845\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pickle\nimport os\n\n# Language tags (used ONLY for prefix IDs, NOT in training data)\nLANG_TAG_HI = \"<2hi>\"\nLANG_TAG_BN = \"<2bn>\"\n\n# Vocab sizes\nSRC_VOCAB_SIZE = 32000\nTGT_VOCAB_SIZE = 50000\n\n# Use full data\nsrc_sample = src_texts\ntgt_sample = tgt_texts\n\n# Prepare combined text (NO language tags in text)\nsrc_combined = \" \".join(src_sample)\ntgt_combined = \" \".join(tgt_sample)\n\nprint(f\"\\nüìè Text lengths:\")\nprint(f\"   Source: {len(src_combined):,} chars\")\nprint(f\"   Target: {len(tgt_combined):,} chars\")\n\n# Delete old tokenizers to force retraining\nimport shutil\nif os.path.exists('tokenizers'):\n    print(\"\\nüóëÔ∏è  Deleting old tokenizers...\")\n    shutil.rmtree('tokenizers')\n    print(\"   Old tokenizers deleted!\")\n\n# Train source tokenizer\nprint(f\"\\nüîß Training source tokenizer (vocab={SRC_VOCAB_SIZE})...\")\nsrc_vocab, src_merges = train_custom_sp_tokenizer(\n    src_combined, \n    SRC_VOCAB_SIZE, \n    add_special_tokens=True\n)\nsrc_tokenizer = CustomTokenizer(src_vocab, src_merges)\n\n# Train target tokenizer (NO language tags in training)\nprint(f\"\\nüîß Training target tokenizer (vocab={TGT_VOCAB_SIZE})...\")\ntgt_vocab, tgt_merges = train_custom_sp_tokenizer(\n    tgt_combined, \n    TGT_VOCAB_SIZE, \n    add_special_tokens=True\n)\n# Add language tags AFTER training for prefix ID approach\ntgt_tokenizer = CustomTokenizer(tgt_vocab, tgt_merges, lang_tags=[LANG_TAG_HI, LANG_TAG_BN])\n\nprint(f\"\\n‚úÖ Tokenizers ready!\")\nprint(f\"   Source vocab: {src_tokenizer.vocab_size}\")\nprint(f\"   Target vocab: {tgt_tokenizer.vocab_size}\")\nprint(f\"   Language tags: {list(tgt_tokenizer.lang_tag_ids.keys())}\")\n\n# Save tokenizers\nos.makedirs(\"tokenizers\", exist_ok=True)\nwith open(\"tokenizers/src_tokenizer.pkl\", \"wb\") as f:\n    pickle.dump((src_vocab, src_merges), f)\nwith open(\"tokenizers/tgt_tokenizer.pkl\", \"wb\") as f:\n    pickle.dump((tgt_vocab, tgt_merges, [LANG_TAG_HI, LANG_TAG_BN]), f)\nprint(\"üíæ Tokenizers saved to tokenizers/\")\n\n# Test tokenization (NO language tags in text)\ntest_src = \"The government announced new policies\"\ntest_hi = \"‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§®‡•á ‡§®‡§à ‡§®‡•Ä‡§§‡§ø‡§Ø‡§æ‡§Ç ‡§ò‡•ã‡§∑‡§ø‡§§ ‡§ï‡•Ä‡§Ç\"  # NO <2hi> tag\ntest_bn = \"‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶®‡¶§‡ßÅ‡¶® ‡¶®‡ßÄ‡¶§‡¶ø ‡¶ò‡ßã‡¶∑‡¶£‡¶æ ‡¶ï‡¶∞‡ßá‡¶õ‡ßá\"  # NO <2bn> tag\n\nprint(\"\\nüß™ Testing tokenization:\")\nprint(f\"Source: {test_src}\")\nsrc_tokens = [src_tokenizer.id2token[i] for i in src_tokenizer.encode(test_src)]\nprint(f\"Tokens: {src_tokens[:15]}...\")\n\nprint(f\"\\nHindi (NO tag in text): {test_hi}\")\nhi_tokens = [tgt_tokenizer.id2token[i] for i in tgt_tokenizer.encode(test_hi)]\nprint(f\"Tokens: {hi_tokens}\")\n\nprint(f\"\\nBengali (NO tag in text): {test_bn}\")\nbn_tokens = [tgt_tokenizer.id2token[i] for i in tgt_tokenizer.encode(test_bn)]\nprint(f\"Tokens: {bn_tokens}\")\n\n# Show how language tags are used as prefix IDs\nprint(f\"\\nüè∑Ô∏è  Language Tag IDs (for prefix):\")\nprint(f\"   {LANG_TAG_HI}: {tgt_tokenizer.get_lang_tag_id(LANG_TAG_HI)}\")\nprint(f\"   {LANG_TAG_BN}: {tgt_tokenizer.get_lang_tag_id(LANG_TAG_BN)}\")\nprint(f\"\\nüí° These IDs are prepended during training:\")\nprint(f\"   Target sequence: [<s>, LANG_TAG_ID, token_ids..., </s>]\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:54:48.221553Z","iopub.execute_input":"2025-11-06T16:54:48.221829Z","iopub.status.idle":"2025-11-06T16:54:59.066444Z","shell.execute_reply.started":"2025-11-06T16:54:48.221812Z","shell.execute_reply":"2025-11-06T16:54:59.065852Z"}},"outputs":[{"name":"stdout","text":"\nüìè Text lengths:\n   Source: 14,504,290 chars\n   Target: 13,656,730 chars\n\nüîß Training source tokenizer (vocab=32000)...\nüîß Training tokenizer with vocab_size=32000...\n   Base vocab: 287 symbols\n   Merges needed: 31709\n   Initial heap size: 5190 pairs\n   Progress: 1000/31709 | Best freq: 49429\n   Progress: 2000/31709 | Best freq: 3257\n   Progress: 3000/31709 | Best freq: 38730\n   Progress: 4000/31709 | Best freq: 4028\n   Progress: 5000/31709 | Best freq: 1743\n   Progress: 6000/31709 | Best freq: 1083\n   Progress: 7000/31709 | Best freq: 18346\n   Progress: 8000/31709 | Best freq: 5377\n   Progress: 9000/31709 | Best freq: 8282\n   Progress: 10000/31709 | Best freq: 2499\n   Progress: 11000/31709 | Best freq: 7294\n   Progress: 12000/31709 | Best freq: 27518\n   Progress: 13000/31709 | Best freq: 603\n   Progress: 14000/31709 | Best freq: 624\n   Progress: 15000/31709 | Best freq: 1903\n   Progress: 16000/31709 | Best freq: 359\n   Progress: 17000/31709 | Best freq: 334\n   Progress: 18000/31709 | Best freq: 9100\n   Progress: 19000/31709 | Best freq: 290\n   Progress: 20000/31709 | Best freq: 1233\n   Progress: 21000/31709 | Best freq: 253\n   Progress: 22000/31709 | Best freq: 236\n   Progress: 23000/31709 | Best freq: 1859\n   Progress: 24000/31709 | Best freq: 1547\n   Progress: 25000/31709 | Best freq: 3725\n   Progress: 26000/31709 | Best freq: 187\n   Progress: 27000/31709 | Best freq: 699\n   Progress: 28000/31709 | Best freq: 9657\n   Progress: 29000/31709 | Best freq: 2322\n   Progress: 30000/31709 | Best freq: 155\n   Progress: 31000/31709 | Best freq: 47713\n\nüìä Vocabulary breakdown:\n   Special tokens: 4\n   Base symbols: 287\n   Merged tokens: 8501\n   Total before truncation: 8792\n   ‚ö†Ô∏è  Warning: Only 8792 tokens generated (target: 32000)\n‚úÖ Final vocabulary: 8792 tokens\n\nüîß Training target tokenizer (vocab=50000)...\nüîß Training tokenizer with vocab_size=50000...\n   Base vocab: 291 symbols\n   Merges needed: 49705\n   Initial heap size: 9644 pairs\n   Progress: 1000/49705 | Best freq: 4338\n   Progress: 2000/49705 | Best freq: 2550\n   Progress: 3000/49705 | Best freq: 4150\n   Progress: 4000/49705 | Best freq: 3641\n   Progress: 5000/49705 | Best freq: 1193\n   Progress: 6000/49705 | Best freq: 9319\n   Progress: 7000/49705 | Best freq: 34333\n   Progress: 8000/49705 | Best freq: 15705\n   Progress: 9000/49705 | Best freq: 697\n   Progress: 10000/49705 | Best freq: 625\n   Progress: 11000/49705 | Best freq: 4336\n   Progress: 12000/49705 | Best freq: 1273\n   Progress: 13000/49705 | Best freq: 481\n   Progress: 14000/49705 | Best freq: 1639\n   Progress: 15000/49705 | Best freq: 602\n   Progress: 16000/49705 | Best freq: 14116\n   Progress: 17000/49705 | Best freq: 809\n   Progress: 18000/49705 | Best freq: 734\n   Progress: 19000/49705 | Best freq: 327\n   Progress: 20000/49705 | Best freq: 2325\n   Progress: 21000/49705 | Best freq: 14412\n   Progress: 22000/49705 | Best freq: 4104\n   Progress: 23000/49705 | Best freq: 261\n   Progress: 24000/49705 | Best freq: 2284\n   Progress: 25000/49705 | Best freq: 759\n   Progress: 26000/49705 | Best freq: 226\n   Progress: 27000/49705 | Best freq: 474\n   Progress: 28000/49705 | Best freq: 921\n   Progress: 29000/49705 | Best freq: 196\n   Progress: 30000/49705 | Best freq: 427\n   Progress: 31000/49705 | Best freq: 5021\n   Progress: 32000/49705 | Best freq: 260\n   Progress: 33000/49705 | Best freq: 640\n   Progress: 34000/49705 | Best freq: 157\n   Progress: 35000/49705 | Best freq: 151\n   Progress: 36000/49705 | Best freq: 146\n   Progress: 37000/49705 | Best freq: 5751\n   Progress: 38000/49705 | Best freq: 4068\n   Progress: 39000/49705 | Best freq: 130\n   Progress: 40000/49705 | Best freq: 126\n   Progress: 41000/49705 | Best freq: 1316\n   Progress: 42000/49705 | Best freq: 17782\n   Progress: 43000/49705 | Best freq: 1249\n   Progress: 44000/49705 | Best freq: 109\n   Progress: 45000/49705 | Best freq: 1488\n   Progress: 46000/49705 | Best freq: 8970\n   Progress: 47000/49705 | Best freq: 2918\n   Progress: 48000/49705 | Best freq: 605\n   Progress: 49000/49705 | Best freq: 1094\n\nüìä Vocabulary breakdown:\n   Special tokens: 4\n   Base symbols: 291\n   Merged tokens: 12989\n   Total before truncation: 13284\n   ‚ö†Ô∏è  Warning: Only 13284 tokens generated (target: 50000)\n‚úÖ Final vocabulary: 13284 tokens\n\n‚úÖ Tokenizers ready!\n   Source vocab: 8792\n   Target vocab: 13286\n   Language tags: ['<2hi>', '<2bn>']\nüíæ Tokenizers saved to tokenizers/\n\nüß™ Testing tokenization:\nSource: The government announced new policies\nTokens: ['‚ñÅTh', 'e', '‚ñÅgovernment', '‚ñÅanno', 'un', 'ce', 'd', '‚ñÅnew', '‚ñÅpolic', 'ie', 's']...\n\nHindi (NO tag in text): ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§®‡•á ‡§®‡§à ‡§®‡•Ä‡§§‡§ø‡§Ø‡§æ‡§Ç ‡§ò‡•ã‡§∑‡§ø‡§§ ‡§ï‡•Ä‡§Ç\nTokens: ['‚ñÅ‡§∏‡§∞‡§ï‡§æ‡§∞', '‚ñÅ‡§®', '‡•á', '‚ñÅ‡§®‡§à', '‚ñÅ‡§®‡•Ä‡§§‡§ø', '‡§Ø‡§æ‡§Ç', '‚ñÅ‡§ò‡•ã‡§∑', '‡§ø‡§§', '‚ñÅ‡§ï‡•Ä', '‡§Ç']\n\nBengali (NO tag in text): ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶®‡¶§‡ßÅ‡¶® ‡¶®‡ßÄ‡¶§‡¶ø ‡¶ò‡ßã‡¶∑‡¶£‡¶æ ‡¶ï‡¶∞‡ßá‡¶õ‡ßá\nTokens: ['‚ñÅ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞', '‚ñÅ‡¶®‡¶§‡ßÅ‡¶®', '‚ñÅ‡¶®‡ßÄ‡¶§‡¶ø', '‚ñÅ‡¶ò‡ßã‡¶∑‡¶£‡¶æ', '‚ñÅ‡¶ï‡¶∞‡ßá‡¶õ‡ßá']\n\nüè∑Ô∏è  Language Tag IDs (for prefix):\n   <2hi>: 13284\n   <2bn>: 13285\n\nüí° These IDs are prepended during training:\n   Target sequence: [<s>, LANG_TAG_ID, token_ids..., </s>]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Example Hindi and Bengali sentences\ntest_hi = f\"‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§®‡•á ‡§®‡§à ‡§®‡•Ä‡§§‡§ø‡§Ø‡§æ‡§Ç ‡§ò‡•ã‡§∑‡§ø‡§§ ‡§ï‡•Ä‡§Ç\"\ntest_bn = f\"‡¶è‡¶á ‡¶ú‡¶æ‡ßü‡¶ó‡¶æ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶≠‡ßÅ‡¶≤‡ßã ‡¶®‡¶æ ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶®‡¶∞‡ßç‡¶Æ‡¶¶‡¶æ ‡¶®‡¶¶‡ßÄ ‡¶Æ‡¶æ‡¶∞‡ßç‡¶¨‡ßá‡¶≤ ‡¶™‡¶æ‡¶•‡¶∞‡ßá‡¶∞ ‡¶™‡¶æ‡¶π‡¶æ‡ßú‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø ‡¶¶‡¶ø‡ßü‡ßá ‡¶™‡ßç‡¶∞‡¶¨‡¶æ‡¶π‡¶ø‡¶§ ‡¶π‡¶ö‡ßç‡¶õ‡ßá ‡¶è‡¶¨‡¶Ç ‡¶®‡¶ø‡¶ú‡ßá‡¶∞ ‡¶∂‡¶æ‡¶®‡ßç‡¶§‡¶ø ‡¶ì ‡¶∏‡ßå‡¶®‡ßç‡¶¶‡¶∞‡ßç‡¶Ø‡¶ï‡ßá ‡¶Ö‡¶®‡¶æ‡¶∏‡¶ï‡ßç‡¶§‡¶ø‡¶§‡ßá ‡¶™‡¶∞‡¶ø‡¶£‡¶§ ‡¶ï‡¶∞‡¶õ‡ßá\"\n\n# Encode\nhi_ids = tgt_tokenizer.encode(test_hi)\nbn_ids = tgt_tokenizer.encode(test_bn)\n\n# Convert IDs back to tokens\nhi_tokens = [tgt_tokenizer.id2token[i] for i in hi_ids]\nbn_tokens = [tgt_tokenizer.id2token[i] for i in bn_ids]\n\nprint(\"Hindi tokens:\", hi_tokens)\nprint(\"Bengali tokens:\", bn_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T03:52:19.654800Z","iopub.execute_input":"2025-11-05T03:52:19.655517Z","iopub.status.idle":"2025-11-05T03:52:19.660767Z","shell.execute_reply.started":"2025-11-05T03:52:19.655492Z","shell.execute_reply":"2025-11-05T03:52:19.660066Z"}},"outputs":[{"name":"stdout","text":"Hindi tokens: ['‚ñÅ‡§∏‡§∞‡§ï‡§æ‡§∞', '‚ñÅ‡§®', '‡•á', '‚ñÅ‡§®‡§à', '‚ñÅ‡§®‡•Ä‡§§‡§ø', '‡§Ø‡§æ‡§Ç', '‚ñÅ‡§ò‡•ã‡§∑', '‡§ø‡§§', '‚ñÅ‡§ï‡•Ä', '‡§Ç']\nBengali tokens: ['‚ñÅ‡¶è‡¶á', '‚ñÅ‡¶ú‡¶æ‡¶Ø‡¶º‡¶ó‡¶æ', '‡¶ó‡ßÅ', '‡¶≤‡ßã', '‚ñÅ‡¶¶‡ßá‡¶ñ‡¶§‡ßá', '‚ñÅ‡¶≠‡ßÅ‡¶≤', '‡ßã', '‚ñÅ‡¶®‡¶æ', '‚ñÅ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá', '‚ñÅ‡¶®‡¶∞', '‡ßç‡¶Æ', '‡¶¶‡¶æ', '‚ñÅ‡¶®‡¶¶‡ßÄ', '‚ñÅ‡¶Æ‡¶æ', '‡¶∞‡ßç‡¶¨', '‡ßá‡¶≤', '‚ñÅ‡¶™‡¶æ‡¶•‡¶∞‡ßá‡¶∞', '‚ñÅ‡¶™‡¶æ‡¶π‡¶æ‡¶°‡¶º‡ßá‡¶∞', '‚ñÅ‡¶Æ‡¶ß‡ßç‡¶Ø', '‚ñÅ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá', '‚ñÅ‡¶™‡ßç‡¶∞‡¶¨‡¶æ‡¶π‡¶ø‡¶§', '‚ñÅ‡¶π‡¶ö‡ßç‡¶õ‡ßá', '‚ñÅ‡¶è‡¶¨‡¶Ç', '‚ñÅ‡¶®‡¶ø‡¶ú‡ßá', '‡¶∞', '‚ñÅ‡¶∂‡¶æ', '‡¶®‡ßç‡¶§', '‡¶ø', '‚ñÅ‡¶ì', '‚ñÅ‡¶∏‡ßå‡¶®‡ßç‡¶¶‡¶∞‡ßç‡¶Ø', '‡¶ï‡ßá', '‚ñÅ‡¶Ö‡¶®‡¶æ', '‡¶∏‡¶ï', '‡ßç‡¶§‡¶ø', '‡¶§‡ßá', '‚ñÅ‡¶™‡¶∞‡¶ø‡¶£‡¶§', '‚ñÅ‡¶ï‡¶∞‡¶õ‡ßá']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class MultilingualNMTDataset(Dataset):\n    \n    def __init__(self, src_texts, tgt_texts, lang_tags, src_tokenizer, tgt_tokenizer, max_len=60):\n        self.src_texts = src_texts\n        self.tgt_texts = tgt_texts\n        self.lang_tags = lang_tags\n        self.src_tokenizer = src_tokenizer\n        self.tgt_tokenizer = tgt_tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.src_texts)\n    \n    def __getitem__(self, idx):\n        src = self.src_texts[idx]\n        tgt = self.tgt_texts[idx]  # NO language tag in text\n        lang_tag = self.lang_tags[idx]\n        \n        # Encode source: BOS + content + EOS\n        src_ids = self.src_tokenizer.encode(src, add_bos=True, add_eos=False)\n        src_ids = src_ids[:self.max_len-1] + [self.src_tokenizer.eos_id]\n        \n        # Get language tag ID\n        lang_tag_id = self.tgt_tokenizer.get_lang_tag_id(lang_tag)\n        \n        # Encode target WITHOUT language tag in text\n        tgt_ids = self.tgt_tokenizer.encode(tgt, add_bos=False, add_eos=False)\n        \n        # Build target: BOS + LANG_TAG + content + EOS\n        tgt_ids = ([self.tgt_tokenizer.bos_id, lang_tag_id] + \n                   tgt_ids[:self.max_len-3] + \n                   [self.tgt_tokenizer.eos_id])\n        \n        return torch.LongTensor(src_ids), torch.LongTensor(tgt_ids)\n\ndef collate_batch(batch):\n    src_batch, tgt_batch = zip(*batch)\n    src_batch = pad_sequence(src_batch, batch_first=True, \n                             padding_value=src_tokenizer.pad_id)\n    tgt_batch = pad_sequence(tgt_batch, batch_first=True, \n                             padding_value=tgt_tokenizer.pad_id)\n    return src_batch, tgt_batch\n\n\ntrain_dataset = MultilingualNMTDataset(\n    src_texts, tgt_texts, lang_tags,\n    src_tokenizer, tgt_tokenizer,\n    max_len=85\n)\n\nBATCH_SIZE = 64\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    collate_fn=collate_batch,\n    num_workers=4,\n    pin_memory=True\n)\n\nprint(f\"\\n‚úÖ Dataset created: {len(train_dataset)} samples\")\nprint(f\"   Batch size: {BATCH_SIZE}\")\nprint(f\"   Batches per epoch: {len(train_loader)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:54:59.067218Z","iopub.execute_input":"2025-11-06T16:54:59.067446Z","iopub.status.idle":"2025-11-06T16:54:59.076290Z","shell.execute_reply.started":"2025-11-06T16:54:59.067429Z","shell.execute_reply":"2025-11-06T16:54:59.075522Z"}},"outputs":[{"name":"stdout","text":"\n‚úÖ Dataset created: 149629 samples\n   Batch size: 64\n   Batches per epoch: 2338\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        pe = torch.zeros(max_len, d_model)\n        position =torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                            (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        \n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:54:59.076982Z","iopub.execute_input":"2025-11-06T16:54:59.077269Z","iopub.status.idle":"2025-11-06T16:54:59.095084Z","shell.execute_reply.started":"2025-11-06T16:54:59.077244Z","shell.execute_reply":"2025-11-06T16:54:59.094365Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class TransformerNMT(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, \n                 nhead=8, num_encoder_layers=4, num_decoder_layers=4,\n                 dim_feedforward=2048, dropout=0.1, max_len=512):\n        super().__init__()\n        \n        self.d_model =d_model\n        self.src_vocab_size =src_vocab_size\n        self.tgt_vocab_size = tgt_vocab_size\n        \n        # Embeddings\n        self.src_embedding =nn.Embedding(src_vocab_size, d_model, padding_idx=0)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model, padding_idx=0)\n        \n        # Positional encoding\n        self.pos_encoder= PositionalEncoding(d_model, max_len, dropout)\n        \n        # Transformer\n        self.transformer = nn.Transformer(\n            d_model=d_model,\n            nhead=nhead,\n            num_encoder_layers=num_encoder_layers,\n            num_decoder_layers=num_decoder_layers,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            activation='gelu',\n            batch_first=True\n        )\n        \n        # Output projection\n        self.fc_out =nn.Linear(d_model, tgt_vocab_size)\n        \n        # Initialize weights\n        self._init_weights()\n    \n    def _init_weights(self):\n        for p in self.parameters():\n            if p.dim()>1:\n                nn.init.xavier_uniform_(p)\n    \n    def forward(self, src, tgt, src_mask=None, tgt_mask=None, \n                src_padding_mask=None, tgt_padding_mask=None):\n        # Embed and add positional encoding\n        src_emb= self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model))\n        tgt_emb = self.pos_encoder(self.tgt_embedding(tgt) * math.sqrt(self.d_model))\n        \n        # Transformer forward\n        output= self.transformer(\n            src_emb, tgt_emb,\n            src_mask=src_mask,\n            tgt_mask=tgt_mask,\n            memory_mask=None,\n            src_key_padding_mask=src_padding_mask,\n            tgt_key_padding_mask=tgt_padding_mask,\n            memory_key_padding_mask=src_padding_mask\n        )\n        \n        return self.fc_out(output)\n    \n    def generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:54:59.095899Z","iopub.execute_input":"2025-11-06T16:54:59.096070Z","iopub.status.idle":"2025-11-06T16:54:59.112005Z","shell.execute_reply.started":"2025-11-06T16:54:59.096051Z","shell.execute_reply":"2025-11-06T16:54:59.111441Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Add this cell to check your data\nsrc_lens = [len(src_tokenizer.encode(s)) for s in src_texts[:1000]]\ntgt_lens = [len(tgt_tokenizer.encode(t)) for t in tgt_texts[:1000]]\n\nprint(f\"Source lengths:\")\nprint(f\"  Mean: {np.mean(src_lens):.1f}\")\nprint(f\"  Median: {np.median(src_lens):.0f}\")\nprint(f\"  95th percentile: {np.percentile(src_lens, 95):.0f}\")\nprint(f\"  Max: {max(src_lens)}\")\n\nprint(f\"\\nTarget lengths:\")\nprint(f\"  Mean: {np.mean(tgt_lens):.1f}\")\nprint(f\"  Median: {np.median(tgt_lens):.0f}\")\nprint(f\"  95th percentile: {np.percentile(tgt_lens, 95):.0f}\")\nprint(f\"  Max: {max(tgt_lens)}\")\n\nprint(f\"\\n% Source truncated at 60: {sum(1 for l in src_lens if l > 83)/len(src_lens)*100:.1f}%\")\nprint(f\"% Source truncated at 100: {sum(1 for l in src_lens if l > 98)/len(src_lens)*100:.1f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T03:52:40.168228Z","iopub.execute_input":"2025-11-05T03:52:40.168951Z","iopub.status.idle":"2025-11-05T03:52:40.259937Z","shell.execute_reply.started":"2025-11-05T03:52:40.168921Z","shell.execute_reply":"2025-11-05T03:52:40.259112Z"}},"outputs":[{"name":"stdout","text":"Source lengths:\n  Mean: 31.4\n  Median: 29\n  95th percentile: 62\n  Max: 127\n\nTarget lengths:\n  Mean: 27.2\n  Median: 24\n  95th percentile: 56\n  Max: 115\n\n% Source truncated at 60: 0.8%\n% Source truncated at 100: 0.3%\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Initialize model\nmodel = TransformerNMT(\n    src_vocab_size=src_tokenizer.vocab_size,\n    tgt_vocab_size=tgt_tokenizer.vocab_size,\n    d_model=512,\n    nhead=8,\n    num_encoder_layers=4,\n    num_decoder_layers=4,\n    dim_feedforward=2048,\n    dropout=0.1\n).to(device)\n\nprint(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:54:59.112910Z","iopub.execute_input":"2025-11-06T16:54:59.113463Z","iopub.status.idle":"2025-11-06T16:55:00.103121Z","shell.execute_reply.started":"2025-11-06T16:54:59.113443Z","shell.execute_reply":"2025-11-06T16:55:00.102339Z"}},"outputs":[{"name":"stdout","text":"\n‚úÖ Model initialized\n   Parameters: 47,547,366\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\nPAD_IDX = tgt_tokenizer.pad_id\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1)\noptimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n\n# Noam scheduler with state_dict support\nclass NoamScheduler:\n    def __init__(self, optimizer, d_model, warmup_steps=4000):\n        self.optimizer = optimizer\n        self.d_model = d_model\n        self.warmup_steps= warmup_steps\n        self.step_num = 0\n    \n    def step(self):\n        self.step_num += 1\n        lr = self.d_model**(-0.5) * min(\n            self.step_num**(-0.5),\n            self.step_num *self.warmup_steps ** (-1.5)\n        )\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n        return lr\n\n    def state_dict(self):\n        return {\n            'd_model': self.d_model,\n            'warmup_steps': self.warmup_steps,\n            'step_num': self.step_num\n        }\n\n    def load_state_dict(self, state_dict):\n        self.d_model = state_dict['d_model']\n        self.warmup_steps = state_dict['warmup_steps']\n        self.step_num = state_dict['step_num']\n\nscheduler = NoamScheduler(optimizer, d_model=512, warmup_steps=4000)\nscaler = torch.amp.GradScaler('cuda')\n\nprint(\"‚úÖ Training setup complete with full checkpoint support\")","metadata":{"id":"Tab3FGP21PCn","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:55:00.103828Z","iopub.execute_input":"2025-11-06T16:55:00.104058Z","iopub.status.idle":"2025-11-06T16:55:02.544411Z","shell.execute_reply.started":"2025-11-06T16:55:00.104042Z","shell.execute_reply":"2025-11-06T16:55:02.543807Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Training setup complete with full checkpoint support\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport numpy as np\nfrom torch import amp\nfrom torch.nn.utils.rnn import pad_sequence\nfrom tqdm import tqdm\n\n# Training history for plots\ntraining_history = {\n    'epoch_loss': [],\n    'epoch_ppl': [],\n    'epoch_time': [],\n    'batch_losses': []\n}\n\nbest_loss = float('inf')\nprevious_ckpt = None  # To track and delete previous checkpoint\n\ndef train_epoch(model, loader, criterion, optimizer, scheduler, scaler, device, epoch, history):\n    model.train()\n    total_loss = 0\n    epoch_start = time.time()\n    \n    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n    \n    for batch_idx, (src, tgt) in enumerate(pbar):\n        src, tgt = src.to(device), tgt.to(device)\n        \n        tgt_input = tgt[:, :-1]\n        tgt_output = tgt[:, 1:]\n        \n        tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n        src_padding_mask = (src == PAD_IDX).to(torch.bool)\n        tgt_padding_mask = (tgt_input == PAD_IDX).to(torch.bool)\n        \n        with amp.autocast('cuda'):\n            output = model(\n                src, tgt_input,\n                tgt_mask=tgt_mask,\n                src_padding_mask = (src == PAD_IDX).float(),\ntgt_padding_mask = (tgt_input == PAD_IDX).float()\n            )\n            loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n        \n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        \n        scheduler.step()\n        lr = optimizer.param_groups[0]['lr']\n        \n        history['batch_losses'].append(loss.item())\n        total_loss += loss.item()\n        pbar.set_postfix({\n            'loss': loss.item(),\n            'ppl': f'{np.exp(loss.item()):.2f}',\n            'lr': f'{lr:.2e}'\n        })\n        \n    epoch_time = time.time() - epoch_start\n    avg_loss = total_loss / len(loader)\n    avg_ppl = np.exp(avg_loss)\n    \n    history['epoch_loss'].append(avg_loss)\n    history['epoch_ppl'].append(avg_ppl)\n    history['epoch_time'].append(epoch_time)\n    \n    return avg_loss\n\n# Training loop\nNUM_EPOCHS = 15\nprint(f\"\\nüöÄ Starting training for {NUM_EPOCHS} epochs...\")\nstart_time = time.time()\n\nfor epoch in range(1, NUM_EPOCHS + 1):\n    avg_loss = train_epoch(model, train_loader, criterion, optimizer, \n                           scheduler, scaler, device, epoch, training_history)\n    avg_ppl = np.exp(avg_loss)\n    print(f\"Epoch {epoch}/{NUM_EPOCHS} - Loss: {avg_loss:.4f} - PPL: {avg_ppl:.2f} - Time: {training_history['epoch_time'][-1]:.2f}s\")\n    \n    # Save current checkpoint\n    ckpt_path = f'model_epoch_{epoch}.pt'\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'scaler_state_dict': scaler.state_dict(),\n        'loss': avg_loss,\n        'history': training_history,\n        'pad_idx': PAD_IDX,\n        'torch_rng_state': torch.get_rng_state(),\n        'cuda_rng_state': torch.cuda.get_rng_state(),\n        'numpy_rng_state': np.random.get_state()\n    }, ckpt_path)\n\n    # Delete previous checkpoint\n    if previous_ckpt and os.path.exists(previous_ckpt):\n        os.remove(previous_ckpt)\n    previous_ckpt = ckpt_path\n\n    # Save best model\n    if avg_loss < best_loss:\n        best_loss = avg_loss\n        torch.save(model.state_dict(), 'best_model.pt')\n\ntotal_training_time = time.time() - start_time\nprint(f\"‚úÖ Training complete! Total time: {total_training_time/3600:.2f} hours\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T03:54:32.087781Z","iopub.execute_input":"2025-11-05T03:54:32.088573Z","iopub.status.idle":"2025-11-05T06:05:38.127424Z","shell.execute_reply.started":"2025-11-05T03:54:32.088541Z","shell.execute_reply":"2025-11-05T06:05:38.126361Z"}},"outputs":[{"name":"stdout","text":"\nüöÄ Starting training for 15 epochs...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:42<00:00,  4.47it/s, loss=5.13, ppl=168.38, lr=4.08e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15 - Loss: 6.2557 - PPL: 520.98 - Time: 522.64s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:42<00:00,  4.47it/s, loss=4.61, ppl=100.54, lr=6.46e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/15 - Loss: 4.8124 - PPL: 123.03 - Time: 522.68s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:43<00:00,  4.47it/s, loss=4.18, ppl=65.44, lr=5.28e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/15 - Loss: 4.2545 - PPL: 70.42 - Time: 523.42s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:43<00:00,  4.47it/s, loss=3.82, ppl=45.77, lr=4.57e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/15 - Loss: 3.8722 - PPL: 48.05 - Time: 523.43s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:43<00:00,  4.47it/s, loss=3.72, ppl=41.27, lr=4.09e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/15 - Loss: 3.6135 - PPL: 37.09 - Time: 523.14s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:42<00:00,  4.47it/s, loss=3.48, ppl=32.49, lr=3.73e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/15 - Loss: 3.4176 - PPL: 30.50 - Time: 522.67s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:43<00:00,  4.46it/s, loss=3.17, ppl=23.90, lr=3.45e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/15 - Loss: 3.2648 - PPL: 26.18 - Time: 523.74s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:43<00:00,  4.47it/s, loss=3.18, ppl=24.16, lr=3.23e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/15 - Loss: 3.1420 - PPL: 23.15 - Time: 523.38s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:44<00:00,  4.46it/s, loss=3.02, ppl=20.57, lr=3.05e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/15 - Loss: 3.0397 - PPL: 20.90 - Time: 524.34s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:45<00:00,  4.45it/s, loss=2.97, ppl=19.52, lr=2.89e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/15 - Loss: 2.9544 - PPL: 19.19 - Time: 525.08s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:43<00:00,  4.47it/s, loss=2.77, ppl=15.98, lr=2.76e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/15 - Loss: 2.8805 - PPL: 17.82 - Time: 523.25s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:41<00:00,  4.48it/s, loss=2.87, ppl=17.64, lr=2.64e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/15 - Loss: 2.8162 - PPL: 16.71 - Time: 521.55s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:41<00:00,  4.48it/s, loss=2.89, ppl=18.01, lr=2.53e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/15 - Loss: 2.7600 - PPL: 15.80 - Time: 521.66s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:42<00:00,  4.48it/s, loss=2.84, ppl=17.07, lr=2.44e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/15 - Loss: 2.7087 - PPL: 15.01 - Time: 522.01s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:42<00:00,  4.47it/s, loss=2.72, ppl=15.21, lr=2.36e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/15 - Loss: 2.6637 - PPL: 14.35 - Time: 522.86s\n‚úÖ Training complete! Total time: 2.19 hours\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport numpy as np\nfrom torch import amp\nfrom torch.nn.utils.rnn import pad_sequence\nfrom tqdm import tqdm\n\n\n# This will be overwritten if a checkpoint is successfully loaded\ntraining_history = {\n    'epoch_loss': [],\n    'epoch_ppl': [],\n    'epoch_time': [],\n    'batch_losses': []\n}\n\nbest_loss = float('inf')\nprevious_ckpt = None  # To track and delete previous checkpoint\n\ndef train_epoch(model, loader, criterion, optimizer, scheduler, scaler, device, epoch, history):\n    model.train()\n    total_loss = 0\n    epoch_start = time.time()\n    \n    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n    \n    for batch_idx, (src, tgt) in enumerate(pbar):\n        src, tgt = src.to(device), tgt.to(device)\n        \n        tgt_input = tgt[:, :-1]\n        tgt_output = tgt[:, 1:]\n        \n        tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n       \n        src_padding_mask_bool = (src == PAD_IDX).to(torch.bool)\n        tgt_padding_mask_bool = (tgt_input == PAD_IDX).to(torch.bool)\n        \n        with amp.autocast('cuda'):\n            output = model(\n                src, tgt_input,\n                tgt_mask=tgt_mask,\n              \n                src_padding_mask = (src == PAD_IDX).float(), \n                tgt_padding_mask = (tgt_input == PAD_IDX).float()\n            )\n            loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n        \n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        \n        scheduler.step()\n        lr = optimizer.param_groups[0]['lr']\n        \n        history['batch_losses'].append(loss.item())\n        total_loss += loss.item()\n        pbar.set_postfix({\n            'loss': loss.item(),\n            'ppl': f'{np.exp(loss.item()):.2f}',\n            'lr': f'{lr:.2e}'\n        })\n        \n    epoch_time = time.time() - epoch_start\n    avg_loss = total_loss / len(loader)\n    avg_ppl = np.exp(avg_loss)\n    \n    history['epoch_loss'].append(avg_loss)\n    history['epoch_ppl'].append(avg_ppl)\n    history['epoch_time'].append(epoch_time)\n    \n    return avg_loss\n\n\nCHECKPOINT_TO_LOAD = '/kaggle/input/nmtmodelbest/model_epoch_15_best.pt'\nSTART_EPOCH = 1\nTOTAL_EPOCHS = 25 \n\n# --- Load from checkpoint ---\nif os.path.exists(CHECKPOINT_TO_LOAD):\n    print(f\"üîÑ Loading checkpoint: {CHECKPOINT_TO_LOAD}\")\n   \n    checkpoint = torch.load(CHECKPOINT_TO_LOAD, map_location=device, weights_only=False)\n    # Load model, optimizer, scheduler, and scaler states\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n    \n    # Restore training state variables\n    START_EPOCH = checkpoint['epoch'] + 1\n    training_history = checkpoint['history']\n    \n    # Restore best_loss from the loaded history\n    if training_history['epoch_loss']:\n        best_loss = min(training_history['epoch_loss'])\n    \n    # Restore RNG states for reproducibility\n    torch.set_rng_state(checkpoint['torch_rng_state'].cpu())\n    torch.cuda.set_rng_state(checkpoint['cuda_rng_state'].cpu())\n    np.random.set_state(checkpoint['numpy_rng_state'])\n    \n    # Set the previous checkpoint path to the one we just loaded\n    previous_ckpt = CHECKPOINT_TO_LOAD\n    \n    print(f\"‚úÖ Checkpoint loaded. Resuming from epoch {START_EPOCH}.\")\n    print(f\"   Current best loss from history: {best_loss:.4f}\")\n\nelse:\n    print(f\"‚ö†Ô∏è Checkpoint '{CHECKPOINT_TO_LOAD}' not found. Starting from scratch.\")\n   \nif START_EPOCH > TOTAL_EPOCHS:\n    print(f\"Model already trained for {START_EPOCH - 1} epochs. No further training needed to reach {TOTAL_EPOCHS} epochs.\")\nelse:\n    print(f\"\\nüöÄ Starting training from epoch {START_EPOCH} to {TOTAL_EPOCHS}...\")\n    start_time = time.time()\n\n    for epoch in range(START_EPOCH, TOTAL_EPOCHS + 1):\n        avg_loss = train_epoch(model, train_loader, criterion, optimizer, \n                               scheduler, scaler, device, epoch, training_history)\n        avg_ppl = np.exp(avg_loss)\n        print(f\"Epoch {epoch}/{TOTAL_EPOCHS} - Loss: {avg_loss:.4f} - PPL: {avg_ppl:.2f} - Time: {training_history['epoch_time'][-1]:.2f}s\")\n        \n        # Save current checkpoint\n        ckpt_path = f'model_epoch_{epoch}.pt'\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'scaler_state_dict': scaler.state_dict(),\n            'loss': avg_loss,\n            'history': training_history,\n            'pad_idx': PAD_IDX,\n            'torch_rng_state': torch.get_rng_state(),\n            'cuda_rng_state': torch.cuda.get_rng_state(),\n            'numpy_rng_state': np.random.get_state()\n        }, ckpt_path)\n\n        # Delete previous checkpoint\n        if previous_ckpt and os.path.exists(previous_ckpt):\n            try:\n                os.remove(previous_ckpt)\n            except OSError as e:\n               print(f\"Warning: Could not delete previous checkpoint '{previous_ckpt}'. Error: {e}\")\n        previous_ckpt = ckpt_path\n\n        # Save best model\n        if avg_loss < best_loss:\n            print(f\"   üéâ New best model found! Loss improved from {best_loss:.4f} to {avg_loss:.4f}. Saving 'best_model.pt'.\")\n            best_loss = avg_loss\n            torch.save(model.state_dict(), 'best_model.pt')\n\n    total_training_time = time.time() - start_time\n    print(f\"‚úÖ Training complete! Total time for this session: {total_training_time/3600:.2f} hours\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:56:25.033370Z","iopub.execute_input":"2025-11-06T16:56:25.033621Z","iopub.status.idle":"2025-11-06T18:07:45.446233Z","shell.execute_reply.started":"2025-11-06T16:56:25.033606Z","shell.execute_reply":"2025-11-06T18:07:45.444971Z"}},"outputs":[{"name":"stdout","text":"üîÑ Loading checkpoint: /kaggle/input/nmtmodelbest/model_epoch_15_best.pt\n‚úÖ Checkpoint loaded. Resuming from epoch 16.\n   Current best loss from history: 2.6637\n\nüöÄ Starting training from epoch 16 to 25...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:48<00:00,  4.42it/s, loss=2.7, ppl=14.94, lr=2.28e-04] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/25 - Loss: 2.6230 - PPL: 13.78 - Time: 528.65s\nWarning: Could not delete previous checkpoint '/kaggle/input/nmtmodelbest/model_epoch_15_best.pt'. Error: [Errno 30] Read-only file system: '/kaggle/input/nmtmodelbest/model_epoch_15_best.pt'\n   üéâ New best model found! Loss improved from 2.6637 to 2.6230. Saving 'best_model.pt'.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:48<00:00,  4.42it/s, loss=2.74, ppl=15.47, lr=2.22e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/25 - Loss: 2.5846 - PPL: 13.26 - Time: 528.44s\n   üéâ New best model found! Loss improved from 2.6230 to 2.5846. Saving 'best_model.pt'.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:48<00:00,  4.43it/s, loss=2.74, ppl=15.43, lr=2.15e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/25 - Loss: 2.5504 - PPL: 12.81 - Time: 528.16s\n   üéâ New best model found! Loss improved from 2.5846 to 2.5504. Saving 'best_model.pt'.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:48<00:00,  4.42it/s, loss=2.61, ppl=13.53, lr=2.10e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/25 - Loss: 2.5188 - PPL: 12.41 - Time: 528.59s\n   üéâ New best model found! Loss improved from 2.5504 to 2.5188. Saving 'best_model.pt'.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:49<00:00,  4.41it/s, loss=2.61, ppl=13.55, lr=2.04e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/25 - Loss: 2.4888 - PPL: 12.05 - Time: 529.85s\n   üéâ New best model found! Loss improved from 2.5188 to 2.4888. Saving 'best_model.pt'.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:48<00:00,  4.42it/s, loss=2.61, ppl=13.60, lr=1.99e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21/25 - Loss: 2.4615 - PPL: 11.72 - Time: 528.37s\n   üéâ New best model found! Loss improved from 2.4888 to 2.4615. Saving 'best_model.pt'.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:49<00:00,  4.42it/s, loss=2.32, ppl=10.14, lr=1.95e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22/25 - Loss: 2.4358 - PPL: 11.42 - Time: 529.22s\n   üéâ New best model found! Loss improved from 2.4615 to 2.4358. Saving 'best_model.pt'.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2338/2338 [08:50<00:00,  4.41it/s, loss=2.42, ppl=11.27, lr=1.91e-04]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23/25 - Loss: 2.4134 - PPL: 11.17 - Time: 530.31s\n   üéâ New best model found! Loss improved from 2.4358 to 2.4134. Saving 'best_model.pt'.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24:   6%|‚ñã         | 148/2338 [00:33<08:17,  4.40it/s, loss=2.32, ppl=10.15, lr=1.90e-04]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3275094558.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTART_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOTAL_EPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         avg_loss = train_epoch(model, train_loader, criterion, optimizer, \n\u001b[0m\u001b[1;32m    146\u001b[0m                                scheduler, scaler, device, epoch, training_history)\n\u001b[1;32m    147\u001b[0m         \u001b[0mavg_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/3275094558.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, criterion, optimizer, scheduler, scaler, device, epoch, history)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             output = model(\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/157802863.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Transformer forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         output = self.transformer(\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0msrc_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_is_causal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         )\n\u001b[0;32m--> 279\u001b[0;31m         output = self.decoder(\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_seq_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m         \u001b[0mtgt_is_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_detect_is_causal_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_is_causal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_detect_is_causal_mask\u001b[0;34m(mask, is_causal, size)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[0;31m# broadcasting the comparison.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcausal_comparison\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m             \u001b[0mmake_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcausal_comparison\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0mmake_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"print(\"\\nüìÇ Loading validation data...\")\nwith open('/kaggle/input/codabenchnmt/test_data1_final.json', 'r', encoding='utf-8') as f:\n    val_data = json.load(f)\n\n# Load best model\ncheckpoint = torch.load('/kaggle/working/best_model.pt')\nmodel.load_state_dict(checkpoint) # <-- Correct\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T18:08:13.658594Z","iopub.execute_input":"2025-11-06T18:08:13.659122Z","iopub.status.idle":"2025-11-06T18:08:14.001755Z","shell.execute_reply.started":"2025-11-06T18:08:13.659085Z","shell.execute_reply":"2025-11-06T18:08:14.000994Z"}},"outputs":[{"name":"stdout","text":"\nüìÇ Loading validation data...\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TransformerNMT(\n  (src_embedding): Embedding(8792, 512, padding_idx=0)\n  (tgt_embedding): Embedding(13286, 512, padding_idx=0)\n  (pos_encoder): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (transformer): Transformer(\n    (encoder): TransformerEncoder(\n      (layers): ModuleList(\n        (0-3): 4 x TransformerEncoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): TransformerDecoder(\n      (layers): ModuleList(\n        (0-3): 4 x TransformerDecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (multihead_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (fc_out): Linear(in_features=512, out_features=13286, bias=True)\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"def translate_batch_greedy(src_texts, target_lang, model, src_tokenizer, tgt_tokenizer, \n                          device, batch_size=32, max_len=120):\n\n    model.eval()\n    all_translations = []\n    \n    lang_tag_id = tgt_tokenizer.get_lang_tag_id(target_lang)\n    \n    # Process in batches with progress bar\n    num_batches = (len(src_texts) + batch_size - 1) // batch_size\n    pbar = tqdm(range(0, len(src_texts), batch_size), \n                total=num_batches, \n                desc=f\"Translating ({batch_size} per batch)\")\n    \n    for i in pbar:\n        batch_texts = src_texts[i:i+batch_size]\n        \n        # Tokenize batch\n        src_ids_list = [src_tokenizer.encode(text, add_bos=True, add_eos=True) for text in batch_texts]\n        src_batch = pad_sequence(\n            [torch.LongTensor(ids) for ids in src_ids_list],\n            batch_first=True,\n            padding_value=src_tokenizer.pad_id\n        ).to(device)\n        \n        with torch.no_grad():\n            # Encode all sources at once\n            # Use .float() to match training code\n            src_padding_mask = (src_batch == src_tokenizer.pad_id).float()\n            src_emb = model.pos_encoder(\n                model.src_embedding(src_batch) * math.sqrt(model.d_model)\n            )\n            memory = model.transformer.encoder(src_emb, src_key_padding_mask=src_padding_mask)\n            \n            # Initialize decoder with BOS + LANG_TAG\n            batch_sz = len(batch_texts)\n            tgt_ids = torch.full((batch_sz, 2), tgt_tokenizer.pad_id, dtype=torch.long, device=device)\n            tgt_ids[:, 0] = tgt_tokenizer.bos_id\n            tgt_ids[:, 1] = lang_tag_id\n            \n            finished = torch.zeros(batch_sz, dtype=torch.bool, device=device)\n            \n            # Greedy decode\n            for step in range(max_len - 2):\n                tgt_emb = model.pos_encoder(\n                    model.tgt_embedding(tgt_ids) * math.sqrt(model.d_model)\n                )\n                \n                # MATCH TRAINING: Use same mask types as training\n                tgt_mask = model.generate_square_subsequent_mask(tgt_ids.size(1)).to(device)\n                \n                # Use .float() for padding masks (same as training)\n                tgt_padding_mask = (tgt_ids == tgt_tokenizer.pad_id).float()\n                memory_padding_mask = (src_batch == src_tokenizer.pad_id).float()\n                \n                output = model.transformer.decoder(\n                    tgt_emb, memory,\n                    tgt_mask=tgt_mask,\n                    tgt_key_padding_mask=tgt_padding_mask,\n                    memory_key_padding_mask=memory_padding_mask\n                )\n                \n                logits = model.fc_out(output[:, -1, :])  # Last position\n                next_tokens = logits.argmax(dim=-1)\n                \n                # Mark finished sequences\n                next_tokens = torch.where(\n                    finished,\n                    torch.full_like(next_tokens, tgt_tokenizer.pad_id),\n                    next_tokens\n                )\n                finished |= (next_tokens == tgt_tokenizer.eos_id)\n                \n                # Append next tokens\n                tgt_ids = torch.cat([tgt_ids, next_tokens.unsqueeze(1)], dim=1)\n                \n                # Stop if all sequences finished\n                if finished.all():\n                    break\n            \n            # Decode all sequences\n            for seq in tgt_ids:\n                translation = tgt_tokenizer.decode(seq.tolist(), skip_special_tokens=True)\n                all_translations.append(translation)\n        \n        # Update progress bar\n        pbar.set_postfix({'total': len(all_translations)})\n    \n    return all_translations\n\nprint(\"‚úÖ Fast batched decoding ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T18:08:18.056529Z","iopub.execute_input":"2025-11-06T18:08:18.057342Z","iopub.status.idle":"2025-11-06T18:08:18.069238Z","shell.execute_reply.started":"2025-11-06T18:08:18.057317Z","shell.execute_reply":"2025-11-06T18:08:18.068357Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Fast batched decoding ready!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"predictions = []\n\n\nprint(\"\\nüîÑ Translating Bengali validation set (batched)...\")\nval_bengali = val_data.get(\"English-Bengali\", {}).get(\"Test\", {})\nbn_ids = list(val_bengali.keys())\nbn_texts = [val_bengali[idx][\"source\"] for idx in bn_ids]\n\nbn_translations = translate_batch_beam(\n    bn_texts, LANG_TAG_BN, model,\n    src_tokenizer, tgt_tokenizer, device,\n    batch_size=32, beam_size=3  \n)\n\nfor idx, translation in zip(bn_ids, bn_translations):\n    predictions.append({\n        'id': int(idx),\n        'prediction': translation\n    })\n\nprint(f\"‚úÖ Bengali done: {len(bn_translations)} translations\")\n\n\nprint(\"\\nüîÑ Translating Hindi validation set (batched)...\")\nval_hindi = val_data.get(\"English-Hindi\", {}).get(\"Test\", {})\nhi_ids = list(val_hindi.keys())\nhi_texts = [val_hindi[idx][\"source\"] for idx in hi_ids]\n\n\nhi_translations = translate_batch_beam(\n    hi_texts, LANG_TAG_HI, model,\n    src_tokenizer, tgt_tokenizer, device,\n    batch_size=32, beam_size=3\n)\n\nfor idx, translation in zip(hi_ids, hi_translations):\n    predictions.append({\n        'id': int(idx),\n        'prediction': translation\n    })\n\n\nimport pandas as pd\ndf = pd.DataFrame(predictions)\n# NO SORTING - keep order as Bengali first, then Hindi\ndf.to_csv('predictions_multilingual.csv', index=False)\nprint(f\"\\n‚úÖ Predictions saved: {len(predictions)} translations\")\nprint(f\"   Order: Bengali first, then Hindi (as in val_data1.json)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T18:33:07.249170Z","iopub.execute_input":"2025-11-06T18:33:07.249803Z"}},"outputs":[{"name":"stdout","text":"\nüîÑ Translating Bengali validation set (batched)...\n","output_type":"stream"},{"name":"stderr","text":"Beam Search (32 sent, 3 beams):  41%|‚ñà‚ñà‚ñà‚ñà      | 251/615 [08:34<14:26,  2.38s/it, total=8032]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def translate_batch_beam(src_texts, target_lang, model, src_tokenizer, tgt_tokenizer, \n                         device, batch_size=16, beam_size=5, max_len=100):\n   \n    \n    model.eval()\n    all_translations = []\n    \n    lang_tag_id = tgt_tokenizer.get_lang_tag_id(target_lang)\n    \n    # Process in batches with progress bar\n    num_batches = (len(src_texts) + batch_size - 1) // batch_size\n    pbar = tqdm(range(0, len(src_texts), batch_size), \n                total=num_batches, \n                desc=f\"Beam Search ({batch_size} sent, {beam_size} beams)\")\n    \n    for i in pbar:\n        batch_texts = src_texts[i:i+batch_size]\n        batch_sz = len(batch_texts)\n        \n        # Tokenize batch\n        src_ids_list = [src_tokenizer.encode(text, add_bos=True, add_eos=True) for text in batch_texts]\n        src_batch = pad_sequence(\n            [torch.LongTensor(ids) for ids in src_ids_list],\n            batch_first=True,\n            padding_value=src_tokenizer.pad_id\n        ).to(device)\n        \n        with torch.no_grad():\n            # Encode sources\n            src_padding_mask = (src_batch == src_tokenizer.pad_id).float()\n            src_emb = model.pos_encoder(\n                model.src_embedding(src_batch) * math.sqrt(model.d_model)\n            )\n            memory = model.transformer.encoder(src_emb, src_key_padding_mask=src_padding_mask)\n            \n            # Expand memory and masks for beam search: (batch_sz*beam_size, seq_len, d_model)\n            memory_beam = memory.unsqueeze(1).repeat(1, beam_size, 1, 1).view(batch_sz * beam_size, -1, model.d_model)\n            src_padding_mask_beam = src_padding_mask.unsqueeze(1).repeat(1, beam_size, 1).view(batch_sz * beam_size, -1)\n            \n            # Initialize beams: (batch_sz, beam_size, seq_len)\n            # Start with [BOS, LANG_TAG]\n            beams = torch.full((batch_sz, beam_size, 2), tgt_tokenizer.pad_id, dtype=torch.long, device=device)\n            beams[:, :, 0] = tgt_tokenizer.bos_id\n            beams[:, :, 1] = lang_tag_id\n            \n            # Beam scores: (batch_sz, beam_size) - log probabilities\n            beam_scores = torch.zeros(batch_sz, beam_size, device=device)\n            beam_scores[:, 1:] = -1e9  # Only first beam is active initially\n            \n            # Track finished beams\n            finished = torch.zeros(batch_sz, beam_size, dtype=torch.bool, device=device)\n            \n            # Decode step by step\n            for step in range(max_len - 2):\n                # Reshape beams for decoding: (batch_sz*beam_size, current_len)\n                current_len = beams.size(2)\n                beams_flat = beams.view(batch_sz * beam_size, current_len)\n                \n                # Embed and decode\n                tgt_emb = model.pos_encoder(\n                    model.tgt_embedding(beams_flat) * math.sqrt(model.d_model)\n                )\n                \n                tgt_mask = model.generate_square_subsequent_mask(current_len).to(device)\n                tgt_padding_mask = (beams_flat == tgt_tokenizer.pad_id).float()\n                \n                output = model.transformer.decoder(\n                    tgt_emb, memory_beam,\n                    tgt_mask=tgt_mask,\n                    tgt_key_padding_mask=tgt_padding_mask,\n                    memory_key_padding_mask=src_padding_mask_beam\n                )\n                \n                # Get logits for next token: (batch_sz*beam_size, vocab_size)\n                logits = model.fc_out(output[:, -1, :])\n                log_probs = F.log_softmax(logits, dim=-1)\n                \n                # Reshape: (batch_sz, beam_size, vocab_size)\n                log_probs = log_probs.view(batch_sz, beam_size, -1)\n                \n                # Add to beam scores: (batch_sz, beam_size, vocab_size)\n                # only allow PAD token\n                vocab_size = log_probs.size(-1)\n                scores = beam_scores.unsqueeze(2) + log_probs\n                \n                # Mask finished beams: force them to generate PAD\n                finished_mask = finished.unsqueeze(2).expand(-1, -1, vocab_size)\n                scores = scores.masked_fill(finished_mask, -1e9)\n                scores[:, :, tgt_tokenizer.pad_id] = scores[:, :, tgt_tokenizer.pad_id].masked_fill(finished, 0)\n                \n                # Flatten and get top beam_size candidates: (batch_sz, beam_size*vocab_size)\n                scores_flat = scores.view(batch_sz, -1)\n                \n                # Get top beam_size candidates\n                top_scores, top_indices = scores_flat.topk(beam_size, dim=1)\n                \n                # Convert flat indices to (beam_idx, token_idx)\n                prev_beam_idx = top_indices // vocab_size  # Which beam did this come from\n                next_token_idx = top_indices % vocab_size  # Which token to append\n                \n                # Update beams\n                # Gather previous beams: (batch_sz, beam_size, current_len)\n                gathered_beams = torch.gather(\n                    beams, 1, \n                    prev_beam_idx.unsqueeze(2).expand(-1, -1, current_len)\n                )\n                \n                # Append new tokens: (batch_sz, beam_size, current_len+1)\n                beams = torch.cat([gathered_beams, next_token_idx.unsqueeze(2)], dim=2)\n                \n                # Update scores\n                beam_scores = top_scores\n                \n                # Update finished status\n                finished = torch.gather(finished, 1, prev_beam_idx)\n                finished |= (next_token_idx == tgt_tokenizer.eos_id)\n                \n                # Early stopping if all beams finished\n                if finished.all():\n                    break\n            \n            #  length penalty (avoid too short translations)\n            # Normalized by length: score / (length ** alpha)\n            # alpha=0.6 i\n            lengths = beams.ne(tgt_tokenizer.pad_id).sum(dim=2).float()  # (batch_sz, beam_size)\n            length_penalty = torch.pow(lengths, 0.6)\n            normalized_scores = beam_scores / length_penalty\n            \n          \n            best_beam_idx = normalized_scores.argmax(dim=1)  # (batch_sz,)\n            best_beams = beams[torch.arange(batch_sz), best_beam_idx]  # (batch_sz, seq_len)\n            \n            # Decode\n            for seq in best_beams:\n                translation = tgt_tokenizer.decode(seq.tolist(), skip_special_tokens=True)\n                all_translations.append(translation)\n        \n        pbar.set_postfix({'total': len(all_translations)})\n    \n    return all_translations\n\nprint(\"‚úÖ Fast batched beam search ready!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T18:31:32.718695Z","iopub.execute_input":"2025-11-06T18:31:32.719475Z","iopub.status.idle":"2025-11-06T18:31:32.735075Z","shell.execute_reply.started":"2025-11-06T18:31:32.719450Z","shell.execute_reply":"2025-11-06T18:31:32.734469Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Fast batched beam search ready!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"USE_BEAM_SEARCH = True  \n\nif USE_BEAM_SEARCH:\n    DECODE_BATCH_SIZE = 32\n    BEAM_SIZE = 5           \n    decode_fn = translate_batch_beam\n    print(f\"\\n‚öôÔ∏è  Using BATCHED BEAM SEARCH (batch={DECODE_BATCH_SIZE}, beams={BEAM_SIZE})\")\nelse:\n    DECODE_BATCH_SIZE = 64 \n    BEAM_SIZE = 1\n    decode_fn = translate_batch_greedy\n    print(f\"\\n‚öôÔ∏è  Using BATCHED GREEDY DECODING (batch={DECODE_BATCH_SIZE}) - FASTEST\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T18:31:37.118293Z","iopub.execute_input":"2025-11-06T18:31:37.118889Z","iopub.status.idle":"2025-11-06T18:31:37.123539Z","shell.execute_reply.started":"2025-11-06T18:31:37.118868Z","shell.execute_reply":"2025-11-06T18:31:37.122675Z"}},"outputs":[{"name":"stdout","text":"\n‚öôÔ∏è  Using BATCHED BEAM SEARCH (batch=32, beams=5)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/working/predictions_multilingual.csv')\n\nprint(f\"\\nüìä Prediction Summary:\")\nprint(f\"   Total predictions: {len(df)}\")\nprint(f\"   ID range: {df['id'].min()} - {df['id'].max()}\")\nprint(f\"   First 5 IDs: {df['id'].head().tolist()}\")\nprint(f\"   Last 5 IDs: {df['id'].tail().tolist()}\")\n\nwith open('answer.csv', 'w', encoding='utf-8') as f:\n  \n    f.write(\"ID\\tTranslation\\n\")\n \n    for _, row in df.iterrows():\n        f.write(f'{row[\"id\"]}\\t\"{row[\"prediction\"]}\"\\n')\n\n\nprint(f\"   Format: Tab-separated with quoted translations\")\nprint(f\"   Order: Preserved from val_data1.json (Bengali ‚Üí Hindi)\")\nprint(f\"   Ready for Codabench upload!\")\n\nwith open('answer.csv', 'r', encoding='utf-8') as f:\n    for i, line in enumerate(f):\n        if i < 3:\n            print(f\"   {line.rstrip()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T18:31:43.667325Z","iopub.execute_input":"2025-11-06T18:31:43.667587Z","iopub.status.idle":"2025-11-06T18:31:45.568765Z","shell.execute_reply.started":"2025-11-06T18:31:43.667570Z","shell.execute_reply":"2025-11-06T18:31:45.568087Z"}},"outputs":[{"name":"stdout","text":"\nüìä Prediction Summary:\n   Total predictions: 42757\n   ID range: 177039 - 563223\n   First 5 IDs: [177039, 177040, 177041, 177042, 177043]\n   Last 5 IDs: [563219, 563220, 563221, 563222, 563223]\n\n‚úÖ Submission file created: answer.csv\n   Format: Tab-separated with quoted translations\n   Order: Preserved from val_data1.json (Bengali ‚Üí Hindi)\n   Ready for Codabench upload!\n\nüîç First 3 lines of answer.csv:\n   ID\tTranslation\n   177039\t\"‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶á‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ó‡ßÅ‡¶≤‡¶ø ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶á‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ó‡ßÅ‡¶≤‡¶ø‡¶§‡ßá ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶á‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ó‡ßÅ‡¶≤‡¶ø ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶á‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ó‡ßÅ‡¶≤‡¶ø‡¶∞ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶á‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ó‡ßÅ‡¶≤‡¶ø‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶á‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ó‡ßÅ‡¶≤‡¶ø‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶π‡¶≤ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶á‡¶≠‡ßá‡¶®‡ßç‡¶ü‡•§\"\n   177040\t\"‡¶≠‡¶ó‡¶¨‡¶æ‡¶® ‡¶¨‡ßç‡¶∞‡¶π‡ßç‡¶Æ‡¶æ ‡¶§‡¶æ‡¶∞ ‡¶§‡¶™‡¶∏‡ßç‡¶Ø‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¶‡¶Ø‡¶º‡¶æ ‡¶ï‡¶∞‡ßá ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶§‡¶æ‡¶Å‡¶ï‡ßá ‡¶¨‡¶≤‡ßá‡¶õ‡¶ø‡¶≤ ‡¶Ø‡ßá ‡¶¨‡¶ø‡¶∑‡ßç‡¶£‡ßÅ‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶¨‡¶ø‡¶¨‡¶æ‡¶π‡ßá‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶§‡¶æ‡¶Å‡¶ï‡ßá ‡¶°‡ßá‡¶Ø‡¶º‡¶æ‡¶®‡¶ï‡ßá ‡¶¨‡¶ø‡¶Ø‡¶º‡ßá ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá ‡•§\"\n","output_type":"stream"}],"execution_count":18}]}